import pandas as pd
import datetime
import time
from typing import Tuple, List, Dict, Any, Optional
from functools import lru_cache

from src.data.data_loader import load_data_for_prediction, get_client_features, get_predicted_news
from src.config import logger, configure_mlflow
from src.train.core import load_model_from_mlflow
from src.predict.constants import CLIENT_FEATURES_COLUMNS, NEWS_FEATURES_COLUMNS


def validate_features(df: pd.DataFrame, required_cols: List[str], source: str) -> None:
    """Valida as colunas necessÃ¡rias no DataFrame"""
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        logger.error("ğŸš¨ [Predict] Colunas ausentes em %s: %s", source, missing)
        raise KeyError(f"Colunas ausentes em {source}: {missing}")
    logger.info("ğŸ‘ [Predict] Todas as colunas necessÃ¡rias foram encontradas em %s.", source)


def build_model_input(
    userId: str, clients_features_df: pd.DataFrame, news_features_df: pd.DataFrame
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    ConstrÃ³i o input final para o modelo baseado no usuÃ¡rio.
    VersÃ£o otimizada para melhorar performance.
    """
    start_time = time.time()

    # ObtÃ©m as features do cliente
    client_feat = get_client_features(userId, clients_features_df)
    if client_feat is None:
        logger.warning("âš ï¸ [Predict] Nenhuma feature encontrada para o usuÃ¡rio %s.", userId)
        return pd.DataFrame(), pd.DataFrame()

    client_get_time = time.time() - start_time
    logger.debug(f"Tempo para obter features do cliente: {client_get_time:.3f}s")

    # Cria o DataFrame do cliente uma Ãºnica vez com apenas as colunas necessÃ¡rias
    client_df = pd.DataFrame([{col: client_feat[col] for col in CLIENT_FEATURES_COLUMNS}])

    # Supondo que todas as notÃ­cias estÃ£o disponÃ­veis para recomendaÃ§Ã£o
    non_viewed = news_features_df.copy()
    if non_viewed.empty:
        logger.warning("âš ï¸ [Predict] Nenhuma notÃ­cia disponÃ­vel para o usuÃ¡rio %s.", userId)
        return pd.DataFrame(), non_viewed

    prep_time = time.time()

    # Extrai apenas as colunas necessÃ¡rias das notÃ­cias (evita cÃ³pia desnecessÃ¡ria de dados)
    news_features = non_viewed[NEWS_FEATURES_COLUMNS].reset_index(drop=True)

    # OtimizaÃ§Ã£o importante: em vez de usar pd.concat para repetir as features do cliente,
    # cria um DataFrame apenas com os valores repetidos usando NumPy
    num_news = len(news_features)

    # Criamos uma lista com os valores de cada coluna repetidos de acordo com o nÃºmero de notÃ­cias
    client_cols = {}
    for col in CLIENT_FEATURES_COLUMNS:
        # Repetimos o valor para cada notÃ­cia
        client_cols[col] = [client_df[col].iloc[0]] * num_news

    # Agora criamos um DataFrame diretamente com essas colunas sem usar concat
    client_features_repeated = pd.DataFrame(client_cols)

    logger.debug(f"Tempo para preparar dataframes: {time.time() - prep_time:.3f}s")

    merge_time = time.time()

    # ConstrÃ³i o DataFrame final combinando client_features_repeated com news_features
    # Em vez de criar um novo DataFrame, atualizamos o existente para evitar cÃ³pia
    final_input = pd.DataFrame()

    # Para cada coluna de cliente
    for col in CLIENT_FEATURES_COLUMNS:
        final_input[col] = client_features_repeated[col]

    # Para cada coluna de notÃ­cia
    for col in NEWS_FEATURES_COLUMNS:
        final_input[col] = news_features[col]

    logger.debug(f"Tempo para merge: {time.time() - merge_time:.3f}s")

    total_time = time.time() - start_time
    logger.info(
        "âœ… [Predict] Input final preparado em %.3fs: %d registros", total_time, len(final_input)
    )

    return final_input, non_viewed


# Otimizamos com cache LRU para evitar processamentos repetidos de campos de data/hora
@lru_cache(maxsize=1024)
def _handle_datetime_fields_cached(date_val, time_val) -> Tuple[Optional[str], Optional[str]]:
    """
    VersÃ£o em cache da funÃ§Ã£o _handle_datetime_fields que trabalha com valores simples
    ao invÃ©s de rows inteiras para melhorar a performance de cache.
    """
    issued_date_str: Optional[str] = None
    issued_time_str: Optional[str] = None

    if date_val is not None:
        if isinstance(date_val, (datetime.date, datetime.datetime)):
            issued_date_str = date_val.isoformat()
        else:
            try:
                issued_date_str = pd.to_datetime(date_val).date().isoformat()
            except Exception:
                issued_date_str = str(date_val)

    if time_val is not None:
        if isinstance(time_val, (datetime.time, datetime.datetime)):
            issued_time_str = time_val.isoformat()
        else:
            try:
                issued_time_str = pd.to_datetime(time_val).time().isoformat()
            except Exception:
                issued_time_str = str(time_val)

    return issued_date_str, issued_time_str


def _handle_datetime_fields(row: pd.Series) -> Tuple[Optional[str], Optional[str]]:
    """
    Wrapper que usa a versÃ£o em cache para processar os campos de data/hora.
    """
    date_val = row.get("issuedDate")
    time_val = row.get("issuedTime")

    # Converte para hashable types se necessÃ¡rio para funcionamento do cache
    if isinstance(date_val, pd.Timestamp):
        date_val = date_val.to_pydatetime()
    if isinstance(time_val, pd.Timestamp):
        time_val = time_val.to_pydatetime()

    return _handle_datetime_fields_cached(date_val, time_val)


def _generate_cold_start_recommendations(
    news_features_df: pd.DataFrame, n: int
) -> List[Dict[str, Any]]:
    """
    VersÃ£o otimizada para gerar recomendaÃ§Ãµes para cold start.
    """
    start_time = time.time()

    # Cria coluna combinada 'issuedDatetime' se possÃ­vel para ordenaÃ§Ã£o
    if "issuedDate" in news_features_df.columns and "issuedTime" in news_features_df.columns:
        # OtimizaÃ§Ã£o: nÃ£o criar cÃ³pia completa do DataFrame, apenas as colunas necessÃ¡rias
        cols_needed = ["pageId", "issuedDate", "issuedTime"]
        if "title" in news_features_df.columns:
            cols_needed.append("title")
        if "url" in news_features_df.columns:
            cols_needed.append("url")

        temp_df = news_features_df[cols_needed].copy()

        # Usa pd.to_datetime mais direto para melhorar performance
        temp_df["issuedDatetime"] = pd.to_datetime(
            temp_df["issuedDate"].astype(str) + " " + temp_df["issuedTime"].astype(str),
            errors="coerce",
        )
        sorted_df = temp_df.sort_values("issuedDatetime", ascending=False).head(n)
    else:
        sorted_df = news_features_df.head(n)

    recommendations = []
    for _, row in sorted_df.iterrows():
        issued_date_str, issued_time_str = _handle_datetime_fields(row)
        recommendations.append(
            {
                "pageId": row["pageId"],
                "score": "desconhecido",
                "title": row.get("title"),
                "url": row.get("url"),
                "issuedDate": issued_date_str,
                "issuedTime": issued_time_str,
            }
        )

    logger.debug(f"Cold start recommendations generated in: {time.time() - start_time:.3f}s")
    return recommendations


def _generate_normal_recommendations(
    scores: List[float],
    non_viewed: pd.DataFrame,
    news_features_df: pd.DataFrame,
    score_threshold: float,
    n: int,
) -> List[Dict[str, Any]]:
    """
    VersÃ£o otimizada para gerar recomendaÃ§Ãµes para usuÃ¡rios nÃ£o cold start.
    """
    start_time = time.time()

    rec_entries = get_predicted_news(scores, non_viewed, n=n, score_threshold=score_threshold)
    recommendations = []

    # OtimizaÃ§Ã£o: criar um dicionÃ¡rio para lookup rÃ¡pido de metadados
    news_lookup = {}
    if len(rec_entries) > 0:
        # Pega apenas os pageIds das recomendaÃ§Ãµes
        recommended_ids = [entry["pageId"] for entry in rec_entries]

        # Filtrar o DataFrame original para obter apenas as linhas necessÃ¡rias
        filtered_news = news_features_df[news_features_df["pageId"].isin(recommended_ids)]

        # Criar lookup table para acesso rÃ¡pido
        for _, row in filtered_news.iterrows():
            news_id = row["pageId"]
            issued_date_str, issued_time_str = _handle_datetime_fields(row)
            news_lookup[news_id] = {
                "title": row.get("title"),
                "url": row.get("url"),
                "issuedDate": issued_date_str,
                "issuedTime": issued_time_str,
            }

    # ConstrÃ³i as recomendaÃ§Ãµes usando o lookup
    for entry in rec_entries:
        news_id = entry["pageId"]
        score = entry.get("score", 0)

        metadata = news_lookup.get(news_id, {})

        recommendations.append(
            {
                "pageId": news_id,
                "score": score,
                "title": metadata.get("title"),
                "url": metadata.get("url"),
                "issuedDate": metadata.get("issuedDate"),
                "issuedTime": metadata.get("issuedTime"),
            }
        )

    logger.debug(f"Normal recommendations generated in: {time.time() - start_time:.3f}s")
    return recommendations


def predict_for_userId(
    userId: str,
    clients_features_df: pd.DataFrame,
    news_features_df: pd.DataFrame,
    model,
    n: int = 5,
    score_threshold: float = 15,
) -> Tuple[List[Dict[str, Any]], bool]:
    """
    VersÃ£o otimizada para realizar a prediÃ§Ã£o e gerar recomendaÃ§Ãµes para o usuÃ¡rio.
    """
    start_total = time.time()

    # Tenta obter as features do cliente
    client_feat = get_client_features(userId, clients_features_df)

    # Se nÃ£o encontrar e o userId tiver tamanho indicativo de hash, assume cold start
    if client_feat is None and len(userId) >= 64:
        logger.info(
            "â„ï¸ [Predict] UsuÃ¡rio %s nÃ£o encontrado (hash vÃ¡lido). Assumindo cold start.", userId
        )
        recommendations = _generate_cold_start_recommendations(news_features_df, n)
        total_time = time.time() - start_total
        logger.info(f"PrediÃ§Ã£o cold start concluÃ­da em {total_time:.3f}s")
        return recommendations, True

    # Fluxo normal de prediÃ§Ã£o
    start_input = time.time()
    final_input, non_viewed = build_model_input(userId, clients_features_df, news_features_df)
    input_time = time.time() - start_input

    if final_input.empty:
        logger.info("ğŸ™ [Predict] Nenhum input construÃ­do para o usuÃ¡rio %s.", userId)
        return [], False

    # MediÃ§Ã£o do tempo de prediÃ§Ã£o do modelo
    start_predict = time.time()
    scores = model.predict(final_input)
    predict_time = time.time() - start_predict

    logger.info(
        "ğŸ”® [Predict] PrediÃ§Ã£o realizada para o usuÃ¡rio %s com %d scores em %.3fs.",
        userId,
        len(scores),
        predict_time,
    )

    # GeraÃ§Ã£o de recomendaÃ§Ãµes
    start_rec = time.time()
    recommendations = _generate_normal_recommendations(
        scores, non_viewed, news_features_df, score_threshold, n
    )
    rec_time = time.time() - start_rec

    total_time = time.time() - start_total
    logger.info(
        "â±ï¸ [Predict] Tempos: input=%.3fs, prediÃ§Ã£o=%.3fs, recomendaÃ§Ãµes=%.3fs, total=%.3fs",
        input_time,
        predict_time,
        rec_time,
        total_time,
    )

    return recommendations, False


def main():
    logger.info("=== ğŸš€ [Predict] Iniciando Pipeline de PrediÃ§Ã£o ===")
    # Carrega os dados via data_loader
    data = load_data_for_prediction()
    news_features_df = data["news_features"]
    clients_features_df = data["clients_features"]

    configure_mlflow()
    model = load_model_from_mlflow()

    userId = "4b3c2c5c0edaf59137e164ef6f7d88f94d66d0890d56020de1ca6afd55b4f297"
    logger.info("=== ğŸš€ [Predict] Processando prediÃ§Ã£o para o usuÃ¡rio: %s ===", userId)

    start_time = time.time()
    recommendations, is_cold_start = predict_for_userId(
        userId, clients_features_df, news_features_df, model
    )
    elapsed = time.time() - start_time

    logger.info("ğŸ¥¶ [Predict] Cold start: %s", is_cold_start)
    logger.info("â±ï¸ [Predict] Tempo total de prediÃ§Ã£o: %.3f segundos", elapsed)

    if recommendations:
        logger.info("ğŸ”” RecomendaÃ§Ãµes:")
        for rec in recommendations:
            pageId = rec["pageId"]
            score = rec["score"]
            logger.info(f"{pageId} : {score}")
    else:
        logger.info("ğŸ˜• [Predict] Nenhuma recomendaÃ§Ã£o gerada para o usuÃ¡rio %s.", userId)

    logger.info("=== âœ… [Predict] Pipeline de PrediÃ§Ã£o Finalizado ===")


if __name__ == "__main__":
    main()
